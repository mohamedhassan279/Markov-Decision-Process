{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohamedhassan279/Markov-Decision-Process/blob/main/Markov_Decision_Process.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "world = np.array([[0, -1, 10], [-1, -1, -1], [-1, -1, -1]], dtype=np.float64)\n",
        "gamma = 0.8\n",
        "epsilon = 1e-9\n",
        "success_prop = 0.8\n",
        "fail_prop = 0.1"
      ],
      "metadata": {
        "id": "lHhPxD61HHvd"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Direction(Enum):\n",
        "    STAY = (0, 0)\n",
        "    UP = (-1, 0)\n",
        "    DOWN = (1, 0)\n",
        "    LEFT = (0, -1)\n",
        "    RIGHT = (0, 1)"
      ],
      "metadata": {
        "id": "zGQM7-MVVGHw"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def isEnd(i, j):\n",
        "    return (i == 0 and j == 2) or (i == 0 and j == 0)\n",
        "\n",
        "\n",
        "def isWall(i, j):\n",
        "    return i < 0 or i > 2 or j < 0 or j > 2"
      ],
      "metadata": {
        "id": "WByfMzfnf9Mu"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_successor(i, j, direction):\n",
        "    successors = []\n",
        "    if not isWall(i + direction.value[0], j + direction.value[1]):\n",
        "        successors.append((i + direction.value[0], j + direction.value[1], success_prop))\n",
        "    if not isWall(i + direction.value[1], j + direction.value[0]):\n",
        "        successors.append((i + direction.value[1], j + direction.value[0], fail_prop))\n",
        "    if not isWall(i - direction.value[1], j - direction.value[0]):\n",
        "        successors.append((i - direction.value[1], j - direction.value[0], fail_prop))\n",
        "    return successors\n",
        "\n",
        "\n",
        "def reward(i, j):\n",
        "    return world[i][j]"
      ],
      "metadata": {
        "id": "eAN-y6u7HuSx"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration():\n",
        "  V = np.zeros_like(world, dtype=np.float64)\n",
        "  optimal_policy = np.array([[Direction.STAY for _ in range(3)] for _ in range(3)])\n",
        "  while True:\n",
        "    delta = 0  # delta = max(|v - v'|)\n",
        "    for i in range(3):\n",
        "      for j in range(3):\n",
        "        if isEnd(i, j):\n",
        "          continue\n",
        "        v = V[i][j]\n",
        "        max_value = float('-inf')\n",
        "        for direction in Direction:\n",
        "            successors = get_successor(i, j, direction)\n",
        "            value = 0\n",
        "            for x, y, prop in successors:\n",
        "                value += prop * (reward(x, y) + gamma * V[x][y])\n",
        "\n",
        "            if value > max_value:\n",
        "                max_value = value\n",
        "                optimal_policy[i][j] = direction\n",
        "        V[i][j] = max_value\n",
        "        delta = max(delta, float(abs(v - V[i][j])))\n",
        "    if delta < epsilon:\n",
        "      break\n",
        "  return V, optimal_policy"
      ],
      "metadata": {
        "id": "qs9wHjw0XMT-"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_policy(policy):\n",
        "  for i in range(3):\n",
        "    for j in range(3):\n",
        "      if policy[i][j] == Direction.UP:\n",
        "          print('↑', end=' ')\n",
        "      elif policy[i][j] == Direction.DOWN:\n",
        "          print('↓', end=' ')\n",
        "      elif policy[i][j] == Direction.LEFT:\n",
        "          print('←', end=' ')\n",
        "      elif policy[i][j] == Direction.RIGHT:\n",
        "          print('→', end=' ')\n",
        "      else:\n",
        "        print('E', end=' ')\n",
        "    print()"
      ],
      "metadata": {
        "id": "rdmDuW8Ug8Ts"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for r in [100, 3, 0, -3]:\n",
        "    world[0][0] = r\n",
        "    optimal_values, optimal_policy = value_iteration()\n",
        "    print('r = ' + str(r))\n",
        "    print(\"optimal values:-\")\n",
        "    print(optimal_values)\n",
        "    print()\n",
        "    print(\"optimal policy:-\")\n",
        "    print_policy(optimal_policy)\n",
        "    print('----------------------------------------------------------------------------------')"
      ],
      "metadata": {
        "id": "t0k7gA-mhVaG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edcee501-a4f7-476e-97d4-fbf397eafcb5"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "r = 100\n",
            "optimal values:-\n",
            "[[ 0.         85.01888433  0.        ]\n",
            " [85.01888433 63.98605414 43.67295431]\n",
            " [57.28454339 47.15571778 32.77349572]]\n",
            "\n",
            "optimal policy:-\n",
            "E ← E \n",
            "↑ ← ← \n",
            "↑ ↑ ← \n",
            "----------------------------------------------------------------------------------\n",
            "r = 3\n",
            "optimal values:-\n",
            "[[0.         8.31716852 0.        ]\n",
            " [2.82806385 5.21460644 8.31716852]\n",
            " [1.1339466  2.79982174 4.64697359]]\n",
            "\n",
            "optimal policy:-\n",
            "E → E \n",
            "→ ↑ ↑ \n",
            "↑ ↑ ↑ \n",
            "----------------------------------------------------------------------------------\n",
            "r = 0\n",
            "optimal values:-\n",
            "[[0.         8.31694028 0.        ]\n",
            " [2.52274391 5.21175346 8.31694028]\n",
            " [1.09027121 2.79445577 4.64639824]]\n",
            "\n",
            "optimal policy:-\n",
            "E → E \n",
            "→ → ↑ \n",
            "→ ↑ ↑ \n",
            "----------------------------------------------------------------------------------\n",
            "r = -3\n",
            "optimal values:-\n",
            "[[0.         8.31692548 0.        ]\n",
            " [2.22058017 5.21156853 8.31692548]\n",
            " [1.06470391 2.79227733 4.6462145 ]]\n",
            "\n",
            "optimal policy:-\n",
            "E → E \n",
            "→ → ↑ \n",
            "→ ↑ ↑ \n",
            "----------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_randomized_policy():\n",
        "    randomized_policy = np.array([[Direction.STAY for _ in range(3)] for _ in range(3)])\n",
        "    directions = [d for d in Direction if d != Direction.STAY]\n",
        "    # Iterate through the array and randomize directions\n",
        "    for i in range(randomized_policy.shape[0]):\n",
        "        for j in range(randomized_policy.shape[1]):\n",
        "            if (i, j) not in [(0, 0), (0, 2)]:\n",
        "                # Exclude STAY and randomize direction\n",
        "                randomized_policy[i, j] = np.random.choice(directions)\n",
        "    return randomized_policy"
      ],
      "metadata": {
        "id": "WzJVUWcFg2iN"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation(policy):\n",
        "    V = np.zeros_like(world, dtype=np.float64)\n",
        "    while True:\n",
        "        delta = 0  # delta = max(|v - v'|)\n",
        "        for i in range(3):\n",
        "            for j in range(3):\n",
        "                if isEnd(i, j):\n",
        "                    continue\n",
        "                v = V[i][j]\n",
        "                successors = get_successor(i, j, policy[i][j])\n",
        "                value = 0\n",
        "                for x, y, prop in successors:\n",
        "                    value += prop * (reward(x, y) + gamma * V[x][y])\n",
        "                V[i][j] = value\n",
        "                delta = max(delta, float(abs(v - V[i][j])))\n",
        "        if delta < epsilon:\n",
        "            break\n",
        "    return V"
      ],
      "metadata": {
        "id": "OKCBvRGkgRRE"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_improvement(V, policy):\n",
        "    for i in range(3):\n",
        "        for j in range(3):\n",
        "            if isEnd(i, j):\n",
        "                continue\n",
        "            max_value = float('-inf')\n",
        "            for direction in Direction:\n",
        "                successors = get_successor(i, j, direction)\n",
        "                value = 0\n",
        "                for x, y, prop in successors:\n",
        "                    value += prop * (reward(x, y) + gamma * V[x][y])\n",
        "\n",
        "                if value > max_value:\n",
        "                    max_value = value\n",
        "                    policy[i][j] = direction"
      ],
      "metadata": {
        "id": "qtm7sezwgUch"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "3KBSFFxdkgyZ"
      },
      "outputs": [],
      "source": [
        "def policy_iteration():\n",
        "    old_policy = generate_randomized_policy()\n",
        "    i = 1\n",
        "    while True:\n",
        "        V = policy_evaluation(old_policy)\n",
        "        new_policy = copy.deepcopy(old_policy)\n",
        "        print(\"iteration \", i)\n",
        "        i += 1\n",
        "        print_policy(new_policy)\n",
        "        policy_improvement(V, new_policy)\n",
        "        if np.array_equal(old_policy, new_policy):\n",
        "            break\n",
        "        old_policy = copy.deepcopy(new_policy)\n",
        "    return V, old_policy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for r in [100, 3, 0, -3]:\n",
        "    world[0][0] = r\n",
        "    optimal_values, optimal_policy = policy_iteration()\n",
        "    print('r = ' + str(r))\n",
        "    print(\"optimal values:-\")\n",
        "    print(optimal_values)\n",
        "    print()\n",
        "    print(\"optimal policy:-\")\n",
        "    print_policy(optimal_policy)\n",
        "    print('----------------------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Upv929CXZYr",
        "outputId": "2c224f50-f4ea-4137-f26f-03590c8b9a52"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration  1\n",
            "E ← E \n",
            "↓ ↓ → \n",
            "↓ → ↑ \n",
            "iteration  2\n",
            "E ← E \n",
            "↑ ↑ ↑ \n",
            "← ↓ → \n",
            "iteration  3\n",
            "E ← E \n",
            "↑ ↑ ← \n",
            "↑ ↑ ↑ \n",
            "iteration  4\n",
            "E ← E \n",
            "↑ ← ← \n",
            "↑ ↑ ← \n",
            "r = 100\n",
            "optimal values:-\n",
            "[[ 0.         85.01888433  0.        ]\n",
            " [85.01888433 63.98605414 43.67295431]\n",
            " [57.28454339 47.15571778 32.77349572]]\n",
            "\n",
            "optimal policy:-\n",
            "E ← E \n",
            "↑ ← ← \n",
            "↑ ↑ ← \n",
            "----------------------------------------------------------------------------------\n",
            "iteration  1\n",
            "E → E \n",
            "← ↓ ↓ \n",
            "↓ ← ↑ \n",
            "iteration  2\n",
            "E → E \n",
            "↑ ↑ ↑ \n",
            "← ↓ ↓ \n",
            "iteration  3\n",
            "E → E \n",
            "→ ↑ ↑ \n",
            "↑ ↑ ↑ \n",
            "r = 3\n",
            "optimal values:-\n",
            "[[0.         8.31716852 0.        ]\n",
            " [2.82806385 5.21460644 8.31716852]\n",
            " [1.1339466  2.79982174 4.64697359]]\n",
            "\n",
            "optimal policy:-\n",
            "E → E \n",
            "→ ↑ ↑ \n",
            "↑ ↑ ↑ \n",
            "----------------------------------------------------------------------------------\n",
            "iteration  1\n",
            "E ↑ E \n",
            "↓ → ↑ \n",
            "← ← ↑ \n",
            "iteration  2\n",
            "E → E \n",
            "→ → ↑ \n",
            "↓ → ↑ \n",
            "iteration  3\n",
            "E → E \n",
            "→ → ↑ \n",
            "→ ↑ ↑ \n",
            "r = 0\n",
            "optimal values:-\n",
            "[[0.         8.31694028 0.        ]\n",
            " [2.52274391 5.21175346 8.31694028]\n",
            " [1.09027121 2.79445577 4.64639824]]\n",
            "\n",
            "optimal policy:-\n",
            "E → E \n",
            "→ → ↑ \n",
            "→ ↑ ↑ \n",
            "----------------------------------------------------------------------------------\n",
            "iteration  1\n",
            "E ← E \n",
            "← ↑ → \n",
            "↑ ← ← \n",
            "iteration  2\n",
            "E → E \n",
            "← → ↑ \n",
            "← ↓ → \n",
            "iteration  3\n",
            "E → E \n",
            "→ → ↑ \n",
            "↓ ↑ ↑ \n",
            "iteration  4\n",
            "E → E \n",
            "→ → ↑ \n",
            "→ ↑ ↑ \n",
            "r = -3\n",
            "optimal values:-\n",
            "[[0.         8.31692548 0.        ]\n",
            " [2.22058017 5.21156853 8.31692548]\n",
            " [1.06470391 2.79227733 4.6462145 ]]\n",
            "\n",
            "optimal policy:-\n",
            "E → E \n",
            "→ → ↑ \n",
            "→ ↑ ↑ \n",
            "----------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}